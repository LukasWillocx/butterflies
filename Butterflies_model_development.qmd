---
title: "Butterflies deep-learning model"
author: "Lukas Willocx"
format: html
editor: visual
---

## Objective

## Model creation

```{r}
#| echo: false
#| include: true
#| warning: false
library(imager)  
library(dplyr)
library(stringr)
library(reticulate)
#use_virtualenv("D:/.virtualenvs/tf2_rocm_env", required = TRUE) #into the v_env
#install_keras(extra_packages = 'tensorflow-directml-plugin',method='virtualenv')
library(keras)
# install_tensorflow(
#   envname = "D:/.virtualenvs/tf2_rocm_env",
#   extra_packages = c("tensorflow-directml-plugin","pandas","numpy","Pillow"),  # Use this for TF 2.10
#   python_version = "3.10.11",
#   restart_session = TRUE)
#install_keras()
library(tensorflow)
```

## Background removal

### Declaring function to remove the background of the images

This function takes all images in one folder and spits them out without a background in another user-defined folder. The output image is a .png regardless since we're handling the absence of a background.

```{r}
remove_background_rembg <- function(input_folder, output_folder) {
  # Load Python modules from python environment r-tf-gpu
  rembg <- import("rembg")
  PIL <- import("PIL.Image")
  os <- import("os")
  
  # Create output directory
  if (!dir.exists(output_folder)) dir.create(output_folder)
  
  # Process images
  images <- list.files(input_folder, pattern = "\\.(jpg|jpeg|png)$", full.names = TRUE)
  
  lapply(images, function(img_path) {
    # Generate output path with PNG extension
    output_filename <- tools::file_path_sans_ext(basename(img_path)) %>% 
      paste0(".png")
    output_path <- file.path(output_folder, output_filename)
    
    # Remove background and save as PNG
    input_image <- PIL$open(img_path)
    output_image <- rembg$remove(input_image)
    output_image$save(output_path, format = "PNG")
  })
}
```

### Performing the removal of the backgrounds

This operation takes a while, the speed at which an image is processed is roughly around one per second. Since the dataset is 6499 large, it takes well over an hour and a half.

```{r}
#| eval: False
#| include: False
#| echo: False
#| include: False
remove_background_rembg(
    input_folder = "D:/data/butterflies/train",
    output_folder = "D:/data/butterflies/train_rembg"
)
```

## identifying labels

```{r}
train_labels<-data.frame(read.csv("D:/data/butterflies/training_set.csv"))
```

Image identifiers to file paths

```{r}
train_labels$path<- paste0("D:/data/butterflies/train_rembg/",
                           paste0(tools::file_path_sans_ext(train_labels$filename), ".png"))
train_labels$label<-as.factor(train_labels$label)

num_classes<-length(unique(train_labels$label))
```

```{r}
datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 30,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  zoom_range = 0.3,
  horizontal_flip = TRUE,
  fill_mode = "nearest",
  validation_split = 0.2
)

train_generator <- flow_images_from_dataframe( #dataframe instead of directory with folders that signify classes
  dataframe = train_labels,
  x_col = 'path',
  y_col = 'label',
  generator = datagen,
  target_size = c(224, 224),
  class_mode = 'categorical',
  subset = 'training',
  batch_size = 32
)

validation_generator <- flow_images_from_dataframe(#dataframe instead of directory with folders that signify classes
  dataframe = train_labels,
  x_col = 'path',
  y_col = 'label',
  generator = datagen,
  target_size = c(224, 224),
  class_mode = 'categorical',
  subset = 'validation',
  batch_size = 32
)
```

## Building the model

```{r}
model <- keras_model_sequential() %>%
  layer_batch_normalization() %>%
  layer_flatten()%>%
  layer_dense(units = num_classes, activation = 'softmax')
```

## Model compilation

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('accuracy')
)
```

## Model training

```{r}
history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = as.integer(train_generator$n / 32),
  epochs = 100,
  validation_data = validation_generator,
  validation_steps = as.integer(validation_generator$n / 32)
)
```

## Model evaluation

```{r}
plot(history)
```

## Make a prediction

```{r}
img <- load.image('test2.jpeg') %>%
  resize(150, 150) %>%
  as.cimg() %>%
  as.array() %>%
  array_reshape(dim = c(1, 150, 150, 3)) %>%
  `/`(255)
```

```{r}
predictions <- model %>% predict(img)

class_indices <- train_generator$class_indices
predicted_index <- which.max(predictions)
predicted_class_name <- names(class_indices)[as.integer(predicted_index)]

# Print result
print(paste("Predicted Class:", predicted_class_name))
```

# Methodology 2

## Transfer training with ResNet-50

```{r}
train_datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 30,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  zoom_range = 0.3,
  horizontal_flip = TRUE,
  fill_mode = "constant",
  validation_split = 0.2
)

val_datagen <- image_data_generator(
  rescale = 1/255,
  validation_split = 0.2
)

train_generator <- flow_images_from_dataframe( #dataframe instead of directory with folders that signify classes
  dataframe = train_labels,
  x_col = 'path',
  y_col = 'label',
  generator = train_datagen,
  target_size = c(224, 224),
  class_mode = 'categorical',
  subset = 'training',
  batch_size = 32,
  shuffle=T
)

validation_generator <- flow_images_from_dataframe(#dataframe instead of directory with folders that signify classes
  dataframe = train_labels,
  x_col = 'path',
  y_col = 'label',
  generator = val_datagen,
  target_size = c(224, 224),
  class_mode = 'categorical',
  subset = 'validation',
  batch_size = 32,
  shuffle=T
)
####################################################################
# Load the pre-trained ResNet50 model, excluding the top layer
base_model <- application_mobilenet_v2(
  weights = "imagenet", 
  include_top = FALSE, 
  input_shape = c(224, 224, 3)
)

# Freeze the base model layers to retain learned features
freeze_weights(base_model)

model <- keras_model_sequential() %>%
  base_model %>%
  layer_global_average_pooling_2d() %>%
  layer_batch_normalization() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = num_classes, activation = "softmax")


# Compile the model (updated learning rate parameter name)
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)

callbacks_list <- list(
  callback_early_stopping(
    monitor = "val_accuracy",
    patience = 10,
    restore_best_weights = TRUE
  )
)

# Train the model (ensure generators are properly defined)
history <- model %>% fit(
  train_generator,
  epochs = 100,
  callbacks = callbacks_list,
  validation_data = validation_generator,
  steps_per_epoch =  ceiling(train_generator$n / 32),
  validation_steps = ceiling(validation_generator$n / 32)
)
```

```{r}
plot(history)
```

```{r}
img <- load.image('test.jpg') %>%
  resize(224, 224) %>%
  as.cimg() %>%
  as.array() %>%
  array_reshape(dim = c(1, 224, 224, 3)) %>%
  `/`(255)
```

```{r}
predictions <- model %>% predict(img)

class_indices <- train_generator$class_indices
predicted_index <- which.max(predictions)
predicted_class_name <- names(class_indices)[as.integer(predicted_index)]

# Print result
print(paste("Predicted Class:", predicted_class_name))
```
